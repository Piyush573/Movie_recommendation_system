# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q4Z335CP9ECvPjtBCeOM9MHDMexiSBIB
"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import CountVectorizer

# Step 1: Load datasets
ratings = pd.read_csv('/content/updated_ratings.csv')  # userId, movieId, rating
movies = pd.read_csv('/content/unique_ids_file.csv')   # movieId, title, genres, cast, crew
sentiments = pd.read_csv('/content/nique_ids_file.csv')  # movieId, Sentiment_score, title

# Step 2: Merge sentiment data into movies DataFrame
movies = movies.merge(sentiments[['movieId', 'Sentiment_score']], on='movieId', how='left')

# Step 3: Preprocess metadata (split genres, cast, and crew into lists)
def preprocess_metadata(movies):
    movies['genres'] = movies['genres'].str.split('|')  # Genres are pipe-separated
    movies['cast'] = movies['cast'].str.split('|')      # Cast is pipe-separated
    movies['crew'] = movies['crew'].str.split('|')      # Crew is pipe-separated
    return movies

movies = preprocess_metadata(movies)

# Step 4: Encode features for content-based filtering
def encode_features(movies):
    movies['combined_features'] = (
        movies['genres'].astype(str) + " " +
        movies['cast'].astype(str) + " " +
        movies['crew'].astype(str)
    )
    return movies

movies = encode_features(movies)

# Step 5: Compute content similarity using Bag of Words (BoW)
def compute_content_similarity_bow(movies):
    vectorizer = CountVectorizer()
    bow_matrix = vectorizer.fit_transform(movies['combined_features'])
    similarity_matrix = cosine_similarity(bow_matrix, bow_matrix)
    return pd.DataFrame(similarity_matrix, index=movies['movieId'], columns=movies['movieId'])

content_similarity = compute_content_similarity_bow(movies)

# Step 6: Compute collaborative similarity (Item-to-Item Cosine Similarity)
user_movie_matrix = ratings.pivot_table(index='userId', columns='movieId', values='rating', fill_value=0)
collaborative_similarity = cosine_similarity(user_movie_matrix.T)  # Transpose to compare movies
collaborative_similarity_df = pd.DataFrame(collaborative_similarity, index=user_movie_matrix.columns, columns=user_movie_matrix.columns)

# Step 7: Compute sentiment similarity based on sentiment scores
def compute_sentiment_similarity(movies):
    sentiment_scores = movies.set_index('movieId')['Sentiment_score']
    max_score = sentiment_scores.max()
    sentiment_matrix = max_score - np.abs(sentiment_scores.values[:, None] - sentiment_scores.values)
    return pd.DataFrame(sentiment_matrix, index=sentiment_scores.index, columns=sentiment_scores.index)

sentiment_similarity = compute_sentiment_similarity(movies)

# Step 8: Prepare training data for neural network
def prepare_nn_data(ratings, content_similarity, collaborative_similarity, sentiment_similarity):
    data = []
    for _, row in ratings.iterrows():
        user_id = row['userId']
        movie_id = row['movieId']
        actual_rating = row['rating']

        # Check if movie_id exists in all similarity matrices
        if movie_id in content_similarity.index and movie_id in collaborative_similarity.index and movie_id in sentiment_similarity.index:
            content_score = content_similarity.loc[movie_id].mean()  # Average similarity score for the movie
            collaborative_score = collaborative_similarity.loc[movie_id].mean()
            sentiment_score = sentiment_similarity.loc[movie_id].mean()
            data.append([content_score, collaborative_score, sentiment_score, actual_rating])

    return pd.DataFrame(data, columns=['content_score', 'collaborative_score', 'sentiment_score', 'actual_rating'])

# Prepare the data
nn_data = prepare_nn_data(ratings, content_similarity, collaborative_similarity_df, sentiment_similarity)
X = nn_data[['content_score', 'collaborative_score', 'sentiment_score']]
y = nn_data['actual_rating']

# Normalize features for better training
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Step 9: Build the neural network
def build_nn():
    model = Sequential([
        Input(shape=(3,)),  # Three inputs: content_score, collaborative_score, sentiment_score
        Dense(16, activation='relu'),  # Hidden layer with 16 neurons
        Dense(8, activation='relu'),   # Another hidden layer with 8 neurons
        Dense(1, activation='linear')  # Output layer for predicting ratings
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])
    return model

# Create the model
nn_model = build_nn()

# Step 10: Train the neural network
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

history = nn_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, verbose=1)

# Recommendation function based on user input
def recommend_movies_by_movie(input_movie, movies, content_similarity, collaborative_similarity, sentiment_similarity, nn_model, scaler, top_n=5):
    if isinstance(input_movie, str):  # If input is a title
        input_movie_id = movies[movies['title'].str.lower() == input_movie.lower()]['movieId'].values
        if len(input_movie_id) == 0:
            print(f"Movie '{input_movie}' not found in the dataset. Please try another title.")
            return None
        input_movie_id = input_movie_id[0]
    else:  # If input is an ID
        input_movie_id = input_movie

    # Check if the input movie exists in all similarity matrices
    if (input_movie_id not in content_similarity.index or
        input_movie_id not in collaborative_similarity.index or
        input_movie_id not in sentiment_similarity.index):
        print(f"Movie ID {input_movie_id} not found in the similarity matrices. Please try another movie.")
        return None

    predicted_scores = []
    for movie_id in content_similarity.index:
        # Ensure movie_id exists in all similarity matrices
        if (movie_id != input_movie_id and
            movie_id in collaborative_similarity.index and
            movie_id in sentiment_similarity.index):
            content_score = content_similarity.loc[input_movie_id, movie_id]
            collaborative_score = collaborative_similarity.loc[input_movie_id, movie_id]
            sentiment_score = sentiment_similarity.loc[input_movie_id, movie_id]

            # Prepare the input for the neural network
            input_features = scaler.transform([[content_score, collaborative_score, sentiment_score]])
            predicted_score = nn_model.predict(input_features)[0][0]
            predicted_scores.append((movie_id, predicted_score))

    predicted_scores = sorted(predicted_scores, key=lambda x: x[1], reverse=True)

    # Get the top N recommended movies
    recommended_movie_ids = [movie_id for movie_id, _ in predicted_scores[:top_n]]
    recommendations = movies[movies['movieId'].isin(recommended_movie_ids)][['movieId', 'title']]

    # Add predicted scores to the recommendations
    recommendations['predicted_score'] = recommendations['movieId'].map(dict(predicted_scores))

    return recommendations.sort_values(by='predicted_score', ascending=False)


# Get user input and provide recommendations
def get_user_input_and_recommendations():
    input_movie = input("Enter a movie title or movieId to get recommendations: ").strip()

    try:
        # Check if the input is a valid integer (movieId)
        input_movie_id = int(input_movie)
        recommendations = recommend_movies_by_movie(
            input_movie=input_movie_id,
            movies=movies,
            content_similarity=content_similarity,
            collaborative_similarity=collaborative_similarity_df,
            sentiment_similarity=sentiment_similarity,
            nn_model=nn_model,
            scaler=scaler,
            top_n=5
        )
    except ValueError:
        # If not a valid integer, treat as a title
        recommendations = recommend_movies_by_movie(
            input_movie=input_movie,
            movies=movies,
            content_similarity=content_similarity,
            collaborative_similarity=collaborative_similarity_df,
            sentiment_similarity=sentiment_similarity,
            nn_model=nn_model,
            scaler=scaler,
            top_n=5
        )

    if recommendations is not None:
        print(f"Top 5 movies similar to '{input_movie}':")
        print(recommendations)

# Call the function to get user input and recommend movies
get_user_input_and_recommendations()

import math  # Import math for RMSE calculation

from sklearn.metrics import mean_squared_error

# Evaluate model performance using RMSE
y_pred = nn_model.predict(X_scaled)
rmse = math.sqrt(mean_squared_error(y, y_pred))
print(f"RMSE of the hybrid recommender system: {rmse:.2f}")

# Function to calculate Top-N accuracy
def calculate_top_n_accuracy(predicted_ratings, ratings, top_n=5, threshold=1.0):
    """
    Calculate the Top-N accuracy for the recommender system based on predicted ratings.

    Parameters:
    - predicted_ratings: List of tuples (movie_id, predicted_rating)
    - ratings: DataFrame containing the actual ratings (userId, movieId, rating)
    - top_n: Number of recommendations to consider
    - threshold: Maximum allowable error for a correct prediction (e.g., within 1 point)

    Returns:
    - accuracy: Proportion of correct predictions within the top N
    """
    correct_predictions = 0
    total_predictions = 0

    # For each user in the ratings data
    for user_id in ratings['userId'].unique():
        user_ratings = ratings[ratings['userId'] == user_id]

        # Get the top N predicted movies for the user
        user_predicted_ratings = [(movie_id, rating) for movie_id, rating in predicted_ratings if movie_id in user_ratings['movieId'].values]
        user_predicted_ratings.sort(key=lambda x: x[1], reverse=True)  # Sort by predicted rating

        top_n_predicted = user_predicted_ratings[:top_n]

        # Compare the top N predicted movies with the actual ratings
        for movie_id, predicted_rating in top_n_predicted:
            actual_rating = user_ratings[user_ratings['movieId'] == movie_id]['rating'].values
            if len(actual_rating) > 0:
                actual_rating = actual_rating[0]
                if abs(predicted_rating - actual_rating) <= threshold:  # Check if predicted rating is within threshold
                    correct_predictions += 1
            total_predictions += 1

    # Calculate accuracy as the proportion of correct predictions
    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
    return accuracy

# Predict ratings using the trained model
predicted_ratings = predict_ratings(content_similarity, collaborative_similarity_df, sentiment_similarity, nn_model, scaler)

# Evaluate Top-N accuracy
top_n_accuracy = calculate_top_n_accuracy(predicted_ratings, ratings, top_n=5, threshold=1.0)
print(f"Top-5 Accuracy of the hybrid recommender system: {top_n_accuracy * 100:.2f}%")